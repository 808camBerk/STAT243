\documentclass{article}
\title{Problem Set 5}
\author{Cameron Adams}


\usepackage{float, hyperref}
\usepackage[margin = 1in]{geometry}
\usepackage{graphicx}
\usepackage{sectsty}
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}
%\SweaveOpts{concordance=TRUE}

\maketitle



<<echo = F>>=
rm(list = ls())

#set working dir
setwd("/Users/CamAdams/repos/STAT243/ps5/")

#load packages
require(RCurl)
require(stringr)
require(pryr)
require(microbenchmark)

#set gobal chunk options
knitr::opts_chunk$set(cache=F,
                      background='#F7F7F7',
                      results='markup')

@

%1
\section{In class we discussed the idea of a closure as a function ...}
 
%2
\section{In class I mentioned that integers as large as $2^{53}$ can be stored exactly in the double precision floating point representation.}


\subsection{Demonstrate how the integers $1, 2, 3, ...,2^{53} - 2, 2^{53} - 1$ can be stored exactly in the $(−1)^S \times 1.d \times 2^{e−1023}$ format where $d$ is represented as 52 bits}

For a double, there are 8 bytes=64 bits to represent the floating point number.
\begin{itemize}
    \item $S$ (1 bits) reprsents the sign of teh number (neg/pos)
    \item $e$ (11 bits) is the base-2 exponent number, $2^11 = 2048$
    \item $d$ (52 bits) the computer accuracy of the real number
\end{itemize}

We can solve for $d$ by rearrange the following equation: $decimal/integer = S \times e \times d$.
Therefore, $1,2,...,2^{53}, 2^{53}-1$ can be represented by:

$$\begin{aligned}
    1 =& (-1)^0 \times 1.0000000000000000 \times 2^{1023 - 1023} \\
    2 =& (-1)^0 \times 1.0000000000000000 \times 2^{1024 - 1023} \\
    2^{53} - 2 =& (-1)^0 \times \frac{2^{53} - 2}{2^{52}} \times 2^{1075 - 1023}  \\
    2^{53} - 1 =& (-1)^0 \times \frac{2^{53} - 1}{2^{52}} \times 2^{1075 - 1023}  \\
    2^{53} =& (-1)^0 \times 1.0000000000000000 \times 2^{1076 - 1023}  \\
\end{aligned}$$

See code below for equalities.
<<>>=
#set digits to higher precision than default
options(digits = 22)

1 == 1*2^(1023-1023)
2 == 1*2^(1024-1023)
2^53 - 2 == ((2^53 - 2) / 2^52) * 2^(1075 - 1023)
2^53 - 1 == ((2^53 - 1) / 2^52) * 2^(1075 - 1023)
2^53 == 1*2^(1076-1023)
@


\subsection{Then show that $2^{53}$ and $2^{53} + 2$ can be represented exactly but $2^{53} + 1$ cannot, so the spacing of numbers of this magnitude is 2.}

These numbers in floating point format are:


$$\begin{aligned}
    2^{53} =& (-1)^0 \times 1.0000000000000000 \times 2^{1076 - 1023}  \\
    2^{53} + 1 =& (-1)^0 \times 1.0000000000000001 \times 2^{1076-1023} \\
    2^{53} + 2 =& (-1)^0 \times 1.0000000000000002 \times 2^{1076-1023} \\
\end{aligned}$$


However, $2^{53} +1$ requires more digits than there is precision. Precision is determined by machine epsilon, $\epsilon$, and absolute spacing is $x\epsilon = 2^{53} \times 2^{-52} = 2$. Therefore, at $2^{53}$ power, we can represenet numbers with spacing == 2. So $2^{53}$ and $2^{53}+2$ work, but $2^{53}+1$ does not.

See code below for equalities.
<<>>=
2^53
2^53 + 1
2^53 + 2

2^53 == 1.000000000000000 * 2^{1076-1023}
2^53 == 1.0000000000000001 * 2^{1076-1023}
2^53 + 1 == 1.0000000000000001 * 2^{1076-1023}
2^53 + 2 == 1.0000000000000002 * 2^{1076-1023}
@

\subsection{Finally show that for numbers starting with $2^{54}$ that the spacing between integers that can be represented exactly is 4.}

This answer is similar to above. We can determine absolute spacing with $x\epsilon = 2^{54} \times 2^{-52} == 4$. 

See code below for equalities.
<<>>=
2^54 
2^54 + 2
2^54 + 4
2^54 == 1.000000000000000 * 2^{1077-1023}
2^54 == 1.0000000000000001 * 2^{1077-1023}
2^54 + 2 == 1.0000000000000001 * 2^{1077-1023}
2^54 + 4 == 1.0000000000000002 * 2^{1077-1023}
@


\section{GPUs tend to use single precision floating point calculations (4 bytes per real number). One advantage of this is that one is moving around half as much data during the process of computations so that should speed up computation, at the expense of precision. Let’s explore a similar question related to integers.}

\subsection{Is it faster to copy a large vector of integers than a numeric vector of the same length in R? Do a bit of experimentation and see what you find.} %3a

<<prob3a>>=
rm(list=ls())

require(dplyr)
require(data.table)

#return to default
options(digits = 7)

#specify integer and numeric vector of same length
x_num <- rnorm(1e7)
x_int <- as.integer(x_num)

x_int_copy <- copy(x_int)
x_num_copy <- copy(x_num)

address(x_int); address(x_int_copy)
address(x_num); address(x_num_copy)

#benchmark copy timing of vectors
require(microbenchmark)
microbenchmark(x_int_copy <- copy(x_int),
               x_num_copy <- copy(x_num),
               times = 20L, unit= "ms")
@
It is much faster (\~3x) to make a copy of the integer vector than numeric vector. This make sense as the integer vector is essentially half the size as the numeric vector, due to the number of bits required to store numeric (64) vs integer (32) numbers.

\subsection{Is it faster to take a subset of size k ≈ n/2 from an integer vector of size n than from a numeric vector of size n?}

<<prob3b>>=

#specify size of subset
k <- length(x_num)/2

#get index of positions to subset, length k
subset <- sample(x = 1:length(x_num), size = k, replace = F)

#benchmark
microbenchmark(x_int[subset],
               x_num[subset],
               times = 20L,
               unit = "ms")
@

It is faster to subset the integer vector than the numeric vector (\~15-20\% faster). 

\section{Let’s consider parallelization of a simple linear algebra computation. In your answer, you can assume m = n/p is an integer.} %4

\subsection{Consider trying to parallelize matrix multiplication, in particular the computation XY where both $X$ and $Y$ are $n \times n$. There are lots of ways we can break up the computations, but let’s keep it simple and consider parallelizing over the columns of $Y$ . Given the considerations we discussed in Unit 7, when you parallelize the matrix multiplication, why might it be better to break up Y into p blocks of m = n/p columns rather than into n individual column-wise computations? Note: I’m not expecting a detailed answer here – a sentence or two is fine.}

<<echo = F, eval = F>>=
n <- 1:5
X <- n %o% n
n <- 5:1
Y <- n %o% n

X * Y
@

In this scenario, over parallelizing (n-idinvidiauls columns) could be slower than parallelizing over $p$ processors. These computations are likley to be relatively fast, and sending each column as a process could introduce lag-time into each parallel process. It would be better to parallelize computations over to the nodes/cores For example, if Y is 40x40 matrix, it would  make sense to divide our  number of processes (40) by the number of cores (4), 40/4=10, and send subsets of 10 columns of Y to be processed by each core. 

\subsection{Let’s consider two ways of parallelizing the computation and count (1) the amount of memory
used at any single moment in time, when all p workers are doing their calculations, including memory use in storing the result and (2) the communication cost – count the total number of numbers that need to be passed to the workers as well as the numbers passed from the workers back to the master when returning the result. Which approach is better for minimizing memory use and which for minimizing communication? Approach A: divide Y into p blocks of equal numbers of columns, where the first block has columns $1, ..., m$ where $m = \frac{n}{p}$ and so forth. Pass $X$ and the $j^{th}$ submatrix of $Y$ to the $j^{th}$ task. Approach B: divide X into p blocks of rows, where the first block has rows 1, ..., m and
Y into p blocks of columns as above. Pass pairs of a submatrix of X and submatrix of Y to
the different tasks.}


Answer depends on the size of the matrix. The second approach has $p^2$ more processes than the first - dividing up both X and Y by m, require us to make sure each row is multiplied by all elements of each column. Additionally, reassembly of complete $XY$ matrix is more difficult in the second approach, because we have $p^2$ more submatrices than in the first appraoch. There are also memory allocations to consider. There may be limited memory available, and having further atomizing operations (second approach vs first approach), may over burden available memeory. I would think the first approach would be best.


\section{Extra Credit}

Base-2 can store numbers in increments of 5, e.g. (5, 0.5, 0.005, 0.001, etc). Therefore, numbers divisible by 5 can be represented by their component parts. So 0.3 + 0.2 == 0.5, 0.3 + 0.7 == 1.0, 0.9 - 0.4 == 0.5, etc. Other numbers, such as 0.3, cannot be exactly reprented by base-2 numbers. Therefore, $0.2 + 0.1 != 0.3$
<<>>=
options(digits = 22)

#0.3 ex.
0.1 + 0.2 == 0.3
0.1 + 0.2 
0.3

#0.5, 1.0 ex.
0.3 + 0.2 == 0.5
0.3 + 0.7 == 1
0.9 + 0.1 == 1
0.9 - 0.4 == 0.5
0.003 + 0.002 == 0.005


@


<<knit,eval=F,echo=F>>=
###########
#knit2pdf
rm(list=ls())
require(knitr)
setwd("/Users/CamAdams/repos/STAT243/ps5/") 
#Sweave2knitr("./ps3.rnw")
knit2pdf(input = "ps5.Rnw", output = "ps5.tex")
@


\end{document}


<<eval = F, echo = F>>=
rm(list=ls())
require(parallel)
require(doParallel)

#create matrices
n <- rnorm(1e3)
X <- n %o% n
n <- rnorm(1e3)
Y <- n %o% n

#divide Y into p blocks of equal numbers of columns
p = 10 #this machine 
m <- length(n) / p
m_ind <- split(seq_along(n), ceiling(seq_along(n) / m))
names(m_ind) <- NULL
#Pass $X$ and the $j^{th}$ submatrix of $Y$ to the $j^{th}$ task

#no approach
XY <- X %*% Y

################
# approach 1

XY_1 <- mclapply(m_ind, mc.cores = 4, FUN = function(x) {
    X %*% Y[, x]
})

XY_1 <- do.call(cbind, XY_1)

################
# approach 2

#create all possible matches between
m_ind_XY <- data.frame(expand.grid(seq_len(length(m_ind)), 
                                   seq_len(length(m_ind))))
#divide X into p blocks of rows, where the first block has rows 
colnames(m_ind_XY) <- c("x_ind", "y_ind")

XY_2a <- mcmapply(x_ind = m_ind_XY$x_ind, y_ind = m_ind_XY$y_ind,
                                  mc.cores = 2,
                    FUN = function(x_ind, y_ind) {
                        X_j <- X[m_ind[[x_ind]], ] #sub jth rows
                        Y_j <- Y[, m_ind[[y_ind]]] #sub jth columns
                        X_j %*% Y_j
                        }, SIMPLIFY = "array") 


#reassemble
XY_2b <- do.call(cbind, lapply(m_ind, function(i)
    do.call(rbind, lapply(i, function(i) XY_2a[,,i]))
))

# check outputs
all.equal(XY, XY_1) 
all.equal(XY_1, XY_2b) 
       
@

<<eval = F, echo = F>>=

#create all possible matches between
m_ind_XY <- data.frame(expand.grid(seq_len(length(m_ind)), 
                                   seq_len(length(m_ind))))
#divide X into p blocks of rows, where the first block has rows 
colnames(m_ind_XY) <- c("x_ind", "y_ind")

bench <- microbenchmark(XY <- X %*% Y,
               
               XY_1 <- do.call(cbind, mclapply(m_ind, mc.cores = 4, function(x) {
                   Y_j <- Y[, x]
                   X %*% Y_j
                   })))#,
               
               {XY_2a <- mcmapply(x_ind = m_ind_XY$x_ind, y_ind = m_ind_XY$y_ind,
                                  mc.cores = 4,
                    FUN = function(x_ind, y_ind) {
                        X_j <- X[m_ind[[x_ind]], ] #sub jth rows
                        Y_j <- Y[, m_ind[[y_ind]]] #sub jth columns
                        X_j %*% Y_j
                        }, SIMPLIFY = "array") ))


                #reassemble
                XY_2b <- do.call(cbind, lapply(list(1:4, 5:8, 9:12, 13:16), 
                                               function(i)
                    do.call(rbind, lapply(i, function(i) XY_2a[,,i]))
                ))},
               times = 10L, unit = "ms")

plot(bench)

dim(XY)
dim(XY_1)
dim(XY_2b)

XY == XY_1
@

